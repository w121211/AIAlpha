{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /tf/AIAlpha\n",
    "# !pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tick bars...\n",
      "Reading data in batches of 20000000\n",
      "Sampling batch 0\n",
      "Creating dollar bars...\n",
      "Reading data in batches of 20000000\n",
      "Sampling batch 0\n",
      "Creating volume bars...\n",
      "Reading data in batches of 20000000\n",
      "Sampling batch 0\n",
      "Processing data...\n",
      "(192711, 6)\n",
      "open\n",
      "high\n",
      "low\n",
      "close\n",
      "volume\n",
      "(192693, 185)\n",
      "                    time     open    high      low   close  volume  open_ret1  \\\n",
      "date                                                                            \n",
      "09/05/2013  02:22:23.748  1640.25  1654.5  1654.25  1654.5    13.0        0.0   \n",
      "09/05/2013  02:22:23.855  1640.25  1654.5  1654.50  1654.5    13.0        0.0   \n",
      "09/05/2013  02:22:24.980  1640.25  1654.5  1654.50  1654.5    13.0        0.0   \n",
      "09/05/2013  02:22:25.001  1640.25  1654.5  1654.50  1654.5    17.0        0.0   \n",
      "09/05/2013  02:22:25.001  1640.25  1654.5  1654.50  1654.5    14.0        0.0   \n",
      "\n",
      "            open_ret2  open_mavg2  open_ewm2  ...     liq_ewm12  liq_ret14  \\\n",
      "date                                          ...                            \n",
      "09/05/2013        0.0         0.0        0.0  ...  29678.698113   0.928993   \n",
      "09/05/2013        0.0         0.0        0.0  ...  28421.744557   1.000454   \n",
      "09/05/2013        0.0         0.0        0.0  ...  27358.168472   1.000302   \n",
      "09/05/2013        0.0         0.0        0.0  ...  27476.373322   0.566838   \n",
      "09/05/2013        0.0         0.0        0.0  ...  26812.777426   0.254622   \n",
      "\n",
      "              liq_mavg14     liq_ewm14  liq_ret16    liq_mavg16     liq_ewm16  \\\n",
      "date                                                                            \n",
      "09/05/2013  34264.303571  30041.337922   0.867060  32771.968750  30231.549017   \n",
      "09/05/2013  34265.000000  28903.626199   1.000454  32772.578125  29205.307956   \n",
      "09/05/2013  34265.464286  27917.609372   0.928993  32669.828125  28299.801138   \n",
      "09/05/2013  32730.214286  27945.461456   1.308285  33084.062500  28279.412769   \n",
      "09/05/2013  27886.857143  27307.799929   1.077249  33187.875000  27677.481855   \n",
      "\n",
      "            liq_ret18    liq_mavg18     liq_ewm18  \n",
      "date                                               \n",
      "09/05/2013   0.812868  32621.888889  30311.696447  \n",
      "09/05/2013   0.565474  31703.680556  29385.044190  \n",
      "09/05/2013   0.867060  31520.472222  28555.934275  \n",
      "09/05/2013   1.308285  31888.680556  28510.730667  \n",
      "09/05/2013   1.000454  31889.263889  27947.811650  \n",
      "\n",
      "[5 rows x 185 columns]\n",
      "                    time     open     high      low    close  volume  \\\n",
      "date                                                                   \n",
      "09/01/2013  17:00:00.101  1640.25  1640.25  1640.25  1640.25    13.0   \n",
      "09/01/2013  17:00:00.101  1640.25  1640.25  1640.25  1640.25    31.0   \n",
      "09/01/2013  17:00:00.101  1640.25  1640.25  1640.25  1640.25    56.0   \n",
      "09/01/2013  17:00:00.101  1640.25  1640.25  1640.25  1640.25    18.0   \n",
      "09/01/2013  17:00:00.101  1640.25  1640.25  1640.25  1640.25    13.0   \n",
      "\n",
      "            open_ret1  open_ret2  open_mavg2  open_ewm2  ...     liq_ewm12  \\\n",
      "date                                                     ...                 \n",
      "09/01/2013        0.0        0.0         0.0        0.0  ...  25126.767588   \n",
      "09/01/2013        0.0        0.0         0.0        0.0  ...  29229.057309   \n",
      "09/01/2013        0.0        0.0         0.0        0.0  ...  39161.157091   \n",
      "09/01/2013        0.0        0.0         0.0        0.0  ...  37640.042430   \n",
      "09/01/2013        0.0        0.0         0.0        0.0  ...  35074.752948   \n",
      "\n",
      "            liq_ret14    liq_mavg14     liq_ewm14  liq_ret16    liq_mavg16  \\\n",
      "date                                                                         \n",
      "09/01/2013   0.928571  24838.071429  25091.148509   0.684211  24501.234375   \n",
      "09/01/2013   2.384615  26946.964286  28733.536818   2.384615  26346.515625   \n",
      "09/01/2013   4.000000  31867.714286  37588.195714   4.000000  30652.171875   \n",
      "09/01/2013   1.200000  32219.196429  36464.811029   1.384615  31164.750000   \n",
      "09/01/2013   0.928571  32102.035714  34367.922038   0.928571  31062.234375   \n",
      "\n",
      "               liq_ewm16  liq_ret18  liq_mavg18     liq_ewm18  \n",
      "date                                                           \n",
      "09/01/2013  25060.578321   1.000000   24968.250  25034.580202  \n",
      "09/01/2013  28364.698411   1.937500   26335.125  28081.151545  \n",
      "09/01/2013  36415.210610   2.947368   29706.750  35513.036283  \n",
      "09/01/2013  35549.386814   1.384615   30162.375  34822.931337  \n",
      "09/01/2013  33776.053747   0.928571   30071.250  33282.623377  \n",
      "\n",
      "[5 rows x 185 columns]\n",
      "(192692,)\n",
      "(192692, 185)\n",
      "(154153,)\n",
      "(154153, 185)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed_data/sample_data/processed_data/autoencoder_data/train_x.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-b5419760e3e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     csv_path=\"sample_data/processed_data/autoencoder_data\", save_csv=True)\n\u001b[1;32m     25\u001b[0m fulldata, y_values, train_x, train_y, test_x, test_y =  preprocess.make_train_test(df_x=df, df_y=None, window=1, \n\u001b[0;32m---> 26\u001b[0;31m csv_path=\"sample_data/processed_data/autoencoder_data\", save_csv=True)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/AIAlpha/data_processor/data_processing.py\u001b[0m in \u001b[0;36mmake_train_test\u001b[0;34m(self, df_x, df_y, window, csv_path, has_y, binary_y, save_csv)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_csv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/processed_data/{csv_path}/train_x.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/processed_data/{csv_path}/train_y.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/processed_data/{csv_path}/test_x.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed_data/sample_data/processed_data/autoencoder_data/train_x.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from data_processor.data_processing import DataProcessing\n",
    "from data_processor.base_bars import BaseBars\n",
    "\n",
    "print('Creating tick bars...')\n",
    "base = BaseBars(\"sample_data/raw_data/price_vol.csv\", \"sample_data/processed_data/price_bars/tick_bars.csv\", \"tick\", 10)\n",
    "base.batch_run()\n",
    "\n",
    "print('Creating dollar bars...')\n",
    "base = BaseBars(\"sample_data/raw_data/price_vol.csv\", \"sample_data/processed_data/price_bars/dollar_bars.csv\", \"dollar\", 20000)\n",
    "base.batch_run()\n",
    "\n",
    "print('Creating volume bars...')\n",
    "base = BaseBars(\"sample_data/raw_data/price_vol.csv\", \"sample_data/processed_data/price_bars/volume_bars.csv\", \"volume\", 50)\n",
    "base.batch_run()\n",
    "\n",
    "print('Processing data...')\n",
    "preprocess = DataProcessing(0.8)\n",
    "df = preprocess.make_features(\n",
    "    file_path=f\"sample_data/processed_data/price_bars/dollar_bars.csv\", window=20,  \n",
    "    csv_path=\"sample_data/processed_data/autoencoder_data\", save_csv=True)\n",
    "fulldata, y_values, train_x, train_y, test_x, test_y =  preprocess.make_train_test(df_x=df, df_y=None, window=1, \n",
    "csv_path=\"sample_data/processed_data/autoencoder_data\", save_csv=True)\n",
    "\n",
    "print('Loading data...')\n",
    "a_train_x = pd.read_csv('sample_data/processed_data/autoencoder_data/train_x.csv', index_col=0)\n",
    "a_train_y = pd.read_csv('sample_data/processed_data/autoencoder_data/train_y.csv', index_col=0)\n",
    "a_test_x = pd.read_csv('sample_data/processed_data/autoencoder_data/test_x.csv', index_col=0)\n",
    "a_test_y = pd.read_csv('sample_data/processed_data/autoencoder_data/test_y.csv', index_col=0)\n",
    "print(a_train_x.head())\n",
    "print(a_train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_data/processed_data/price_bars/dollar_bars.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>time</td>\n",
       "      <td>open</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>close</td>\n",
       "      <td>volume</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      time  open  high  low  close  volume\n",
       "date                                      \n",
       "date  time  open  high  low  close  volume"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = df.columns[1]\n",
    "# df[\"open\"].eq(\"open\").any()\n",
    "df.loc[df['open']=='open']\n",
    "# df[i]\n",
    "# df.iloc[0, 1]\n",
    "# df[i].astype(float)\n",
    "# df[f'{i}_ret1'] = np.log(df[i].astype(float)/float(df[i].shift(1)))\n",
    "# df[f'{i}_ret1'] = np.log(df[i]/df[i].shift(1))\n",
    "# df[i] / df[i]\n",
    "# df.iloc[105, 1:]\n",
    "# df[\"aaa\"] = np.log(df[i]/df[i].shift(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
